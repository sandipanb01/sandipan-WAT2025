# -*- coding: utf-8 -*-
"""Sandipan gemma 3 1b doc translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LqNZN6RxdOwSgCJeHKA6UfGGJSdrGXPo
"""

import json
from pathlib import Path
from typing import List

from datasets import load_dataset
from tqdm.auto import tqdm

PAIRS: List[str] = [
    "eng_ben", "eng_guj", "eng_hin", "eng_kan", "eng_mal",
    "eng_mar", "eng_ori", "eng_pan", "eng_tam", "eng_tel", "eng_urd",
]

def save_jsonl(lines: List[str], path: Path) -> None:
    """Saves a list of strings to a JSONL file."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for ln in lines:
            json.dump([ln], f, ensure_ascii=False)
            f.write("\n")

def dump_pair(subset: str, pair: str, out_root: Path) -> None:
    """Downloads and saves a specific language pair from a subset."""
    src_lang, tgt_lang = pair.split("_")
    ds = load_dataset("ai4bharat/Pralekha", subset, split=f"{src_lang}_{tgt_lang}")
    print(f"  {subset}/{pair}: {len(ds):,} docs")
    save_jsonl(ds["src_txt"], out_root / subset / pair / f"doc.{src_lang}.jsonl")
    save_jsonl(ds["tgt_txt"], out_root / subset / pair / f"doc.{tgt_lang}.jsonl")

def main() -> None:
    """Main function to parse arguments and download data."""
    # Define output directory and splits directly for Colab environment
    out_root = Path("./pralekha_data").expanduser()  # Example output directory
    splits = ["dev", "test"]  # Example splits

    print("Language pairs fixed to:", ", ".join(PAIRS))
    print("Splits:", ", ".join(splits))

    for subset in splits:
        for pair in tqdm(PAIRS, desc=f"[{subset}]"):
            dump_pair(subset, pair, out_root)

    print("\n✓ All data has been written to", out_root.resolve())

if __name__ == "__main__":
    main()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Generate few-shot prompts for Gemma 3 models.
Output: doc.{src}_2_{tgt}.{k}.jsonl
"""
from __future__ import annotations
import argparse, json, sys
from pathlib import Path
from typing import List

LANG_LABELS = {
    "eng": "English",   "ben": "Bengali",   "guj": "Gujarati",
    "hin": "Hindi",     "kan": "Kannada",   "mal": "Malayalam",
    "mar": "Marathi",   "ori": "Odia",      "pan": "Punjabi",
    "tam": "Tamil",     "tel": "Telugu",    "urd": "Urdu",
}

# ───────────────────────── helper funcs ─────────────────────────
def _extract_text(line: str) -> str:
    obj = json.loads(line)
    if isinstance(obj, list):
        return str(obj[0])
    if isinstance(obj, str):
        return obj
    raise ValueError("Line must be string or single-element list.")

def _load_texts(path: Path) -> List[str]:
    with path.open(encoding="utf-8") as f:
        return [_extract_text(ln).strip() for ln in f if ln.strip()]

def _build_block(src, tgt, src_lbl, tgt_lbl):
    return f"<start_of_turn>user\nTranslate this {src_lbl} text to {tgt_lbl}:\n\n{src}<end_of_turn>\n<start_of_turn>model\n{tgt}<end_of_turn>"

def _build_final_prompt(src, src_lbl, tgt_lbl):
    return f"<start_of_turn>user\nTranslate this {src_lbl} text to {tgt_lbl}:\n\n{src}<end_of_turn>\n<start_of_turn>model\n"

def _default_out_path(test_path: Path, src: str, tgt: str, k: int) -> Path:
    return test_path.parent / f"doc.{src}_2_{tgt}.{k}.jsonl"

def _word_len(txt: str) -> int:
    return len(txt.split())

# ─────────────────────────── main ───────────────────────────────
def main() -> None:
    # Define parameters directly for Colab environment
    test_file = "./pralekha_data/test/eng_hin/doc.eng.jsonl"  # Example test file
    example_src_file = "./pralekha_data/dev/eng_hin/doc.eng.jsonl" # Example example source file
    example_tgt_file = "./pralekha_data/dev/eng_hin/doc.hin.jsonl" # Example example target file
    few_shot = 5  # Example number of few-shot examples
    src_lang = "eng" # Example source language
    tgt_lang = "hin" # Example target language
    output_file = None # Optional output file
    src_label = None # Optional source label
    tgt_label = None # Optional target label


    if few_shot < 0:
        sys.exit("few_shot must be ≥ 0")

    src_lbl = src_label or LANG_LABELS.get(src_lang, src_lang)
    tgt_lbl = tgt_label or LANG_LABELS.get(tgt_lang, tgt_lang)

    test_path   = Path(test_file)
    ex_src_path = Path(example_src_file)
    ex_tgt_path = Path(example_tgt_file)

    out_path = (
        Path(output_file)
        if output_file
        else _default_out_path(test_path, src_lang, tgt_lang, few_shot)
    )
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # ---- load all example pairs ---------------------------------------------
    ex_src = _load_texts(ex_src_path)
    ex_tgt = _load_texts(ex_tgt_path)
    if len(ex_src) != len(ex_tgt):
        sys.exit("[!] Example source / target length mismatch")

    # ---- pick first k whose English side is 101-200 words --------------------
    selected = []
    for s, t in zip(ex_src, ex_tgt):
        eng_side = s if src_lang == "eng" else t if tgt_lang == "eng" else None
        if eng_side is None:
            # neither side is English – should not happen for this task
            continue
        wlen = _word_len(eng_side)
        if 100 < wlen <= 200:
            selected.append((s, t))
            if len(selected) == few_shot:
                break

    if len(selected) < few_shot:
        print(f"[!] Only found {len(selected)} examples satisfying 101-200 words (needed {few_shot})")

    # Build system prompt
    system_prompt = f"<start_of_turn>system\nYou are a professional translator. Translate the text accurately from {src_lbl} to {tgt_lbl}, preserving meaning, tone, and formatting.<end_of_turn>\n"

    # Build example blocks
    example_blocks = [system_prompt]
    for s, t in selected:
        example_blocks.append(_build_block(s, t, src_lbl, tgt_lbl))

    # ---- build prompts -------------------------------------------------------
    with test_path.open(encoding="utf-8") as fin, out_path.open("w", encoding="utf-8") as fout:
        tests = 0
        for raw in fin:
            if not raw.strip():
                continue
            test_src = _extract_text(raw).strip()
            final_prompt = _build_final_prompt(test_src, src_lbl, tgt_lbl)
            full_prompt = "\n".join(example_blocks + [final_prompt])
            fout.write(json.dumps([full_prompt], ensure_ascii=False) + "\n")
            tests += 1

    print(f"✓ {out_path} written. examples={len(selected)}, tests={tests}")

if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade "datasets>=2.19.0" "vllm>=0.4.2" accelerate torch --extra-index-url https://download.pytorch.org/whl/cu121

#!/usr/bin/env python
"""
Quick-start: translate a slice of the Pralekha dev-set with vLLM + Gemma-3-1b-pt.
"""
import os
# os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID" # Removing this line
from pathlib import Path
from typing import List

from datasets import load_dataset
from vllm import LLM, SamplingParams

# Removing imports related to fixing cuda visible devices: import os, re, subprocess, sys

# Removing the _fix_cuda_visible_devices function
# def _fix_cuda_visible_devices():
#     cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "")
#     if not cvd.startswith("GPU-"):
#         # Already numeric -> nothing to do
#         return

#     # 1) Build a map {UUID -> index} from `nvidia-smi -L`
#     try:
#         smi = subprocess.check_output(["nvidia-smi", "-L"], text=True)
#     except FileNotFoundError:
#         sys.exit("!! nvidia-smi not found—cannot map GPU UUIDs to indices")

#     uuid2idx = {}
#     for line in smi.splitlines():
#         # Example: "GPU 2: NVIDIA H100 (UUID: GPU-b1b8e0d1-5d40-8a62-d5da-393bca3fd881)"
#         m = re.match(r"GPU\s+(\d+):.*\(UUID:\s+(GPU-[0-9a-f\-]+)\)", line)
#         if m:
#             idx, uuid = m.groups()
#             uuid2idx[uuid] = idx

#     # 2) Translate the job-allocated UUID list to indices
#     try:
#         new_ids = ",".join(uuid2idx[uuid] for uuid in cvd.split(","))
#     except KeyError as e:
#         missing = str(e).strip("'")
#         sys.exit(f"!! UUID {missing} not found in `nvidia-smi -L` output.\n"
#                  f"   CUDA_VISIBLE_DEVICES was: {cvd}")

#     os.environ["CUDA_VISIBLE_DEVICES"] = new_ids
#     print(f"[fix-gpu] CUDA_VISIBLE_DEVICES  {cvd}  →  {new_ids}")

# Removing the call to _fix_cuda_visible_devices
# _fix_cuda_visible_devices()

# ---------------------------------------------------------------------------
# 1. DATA LOADING
# ---------------------------------------------------------------------------
def load_pralekha_split(
    src_lang: str = "eng",
    tgt_lang: str = "hin",
    subset: str = "dev",
    max_rows: int | None = 100,
):
    ds = load_dataset("ai4bharat/Pralekha", f"{subset}", split=f"{src_lang}_{tgt_lang}")
    if max_rows:
        ds = ds.select(range(min(max_rows, len(ds))))
    return ds

# ---------------------------------------------------------------------------
# 2. PROMPT TEMPLATING (Gemma format)
# ---------------------------------------------------------------------------
_SYSTEM = (
    "You are a professional translator. Translate the user message precisely "
    "from {src} to {tgt}, preserving meaning, tone, and any markup."
)

def make_prompts(sentences: List[str], src: str, tgt: str) -> List[str]:
    sys = _SYSTEM.format(src=src, tgt=tgt)
    return [
        (
            f"<start_of_turn>user\n"
            f"{sys}\n"
            f"Please translate the following text from {src} to {tgt}:\n"
            f"{s}\n"
            f"<end_of_turn>\n"
            f"<start_of_turn>model\n"
        )
        for s in sentences
    ]


# ---------------------------------------------------------------------------
# 3. MODEL INSTANTIATION (vLLM)
# ---------------------------------------------------------------------------
def init_gemma(checkpoint: str = "google/gemma-3-1b-pt") -> LLM:
    """
    Loads the Gemma 3 1B PT checkpoint under vLLM.
    """
    return LLM(
        model=checkpoint,
        dtype="float16", # Changed from bfloat16 to float16 for Tesla T4 compatibility
        tokenizer=checkpoint,
        max_model_len=8192,  # Increased for document-level translation
    )


# ---------------------------------------------------------------------------
# 4. BATCH TRANSLATION
# ---------------------------------------------------------------------------
def translate(
    llm: LLM,
    prompts: List[str],
    temperature: float = 0.0,
    max_tokens: int = 4096,  # Changed to 4096
) -> List[str]:
    params = SamplingParams(temperature=temperature, max_tokens=max_tokens)
    outputs = llm.generate(prompts, params)
    # vLLM returns a list of RequestOutput; we take the first candidate
    return [out.outputs[0].text.strip() for out in outputs]


# ---------------------------------------------------------------------------
# 5. END-TO-END EXECUTION
# ---------------------------------------------------------------------------
def main():
    SRC, TGT = "eng", "hin"          # change here for other pairs
    SUBSET = "dev"                   # or "train" / "test"
    N = 50                           # quick smoke-test
    MODEL = "google/gemma-3-1b-it" # Changed model to instruct following cell NlbMmAKflT6n

    ds = load_pralekha_split(SRC, TGT, SUBSET, N)
    print(f"Loaded {len(ds):,} rows from {SUBSET}/{SRC}_{TGT}")

    llm = init_gemma(MODEL)
    prompts = make_prompts(ds["src_txt"], SRC, TGT)
    translations = translate(llm, prompts)

    # Add predictions & persist
    ds = ds.add_column("pred_txt", translations)
    out_file = Path(f"translations_{MODEL.split('/')[-1]}_{SRC}_{TGT}_{SUBSET}_{N}.csv")
    ds.to_pandas().to_csv(out_file, index=False)
    print(f"✓ Saved translations to {out_file.resolve()}")

if __name__ == "__main__":
    main()

#!/usr/bin/env python
# inference_vllm.py
import argparse
import json
import sys
import os
from tqdm import tqdm
from pathlib import Path
import torch

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--input_file", required=True)
    p.add_argument("--output_file", required=True)
    p.add_argument("--model", default="google/gemma-3-1b-pt",
                   help="HF repo path or local dir; using google/gemma-3-1b-pt")
    p.add_argument("--max_new_tokens", type=int, default=4096)
    p.add_argument("--sampling", action="store_true")
    p.add_argument("--temperature", type=float, default=0.7)
    p.add_argument("--top_p", type=float, default=0.9)
    p.add_argument("--batch_size", type=int, default=4)
    return p.parse_args()

def load_prompts(path):
    for line in Path(path).read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line: continue
        obj = json.loads(line) if line[0] in "{[" else {"prompt": line}
        if isinstance(obj, list): yield obj[0]
        else: yield obj["prompt"]

def build_generator(args):
    # Import vLLM here to control error handling
    from vllm import LLM, SamplingParams

    # Initialize vLLM engine with safe settings for problematic environments
    print(f"Initializing vLLM for model: {args.model}")
    llm = LLM(
        model=args.model,
        tensor_parallel_size=1,  # Use single GPU tensor parallelism for stability
        dtype="float16", # Changed from bfloat16 to float16 for Tesla T4 compatibility
        trust_remote_code=True,
        gpu_memory_utilization=0.85,  # Be conservative with memory
    )

    # Configure sampling parameters
    sampling_params = SamplingParams(
        max_tokens=args.max_new_tokens,
        temperature=args.temperature if args.sampling else 0.0,
        top_p=args.top_p if args.sampling else 1.0,
        stop=["\n\n"],
    )

    # Define a generator function that uses vLLM for batched inference
    def generate(prompts):
        outputs = llm.generate(prompts, sampling_params)
        return [
            {"generated_text": prompt + output.outputs[0].text}
            for prompt, output in zip(prompts, outputs)
        ]

    return generate

def main():
    # Define parameters directly for Colab environment
    input_file = "./pralekha_data/test/eng_hin/doc.eng_2_hin.5.jsonl" # Example input file
    output_file = "./pralekha_data/test/eng_hin/translations.jsonl" # Example output file
    model = "google/gemma-3-1b-pt"  # Example model


    # Remove argparse logic
    # args = parse_args()
    class Args:
        def __init__(self, input_file, output_file, model, max_new_tokens=4096, sampling=False, temperature=0.7, top_p=0.9, batch_size=4):
            self.input_file = input_file
            self.output_file = output_file
            self.model = model
            self.max_new_tokens = max_new_tokens
            self.sampling = sampling
            self.temperature = temperature
            self.top_p = top_p
            self.batch_size = batch_size

    args = Args(input_file=input_file, output_file=output_file, model=model)

    prompts = list(load_prompts(args.input_file))
    print(f"Loaded {len(prompts)} prompts.")

    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)

    # Initialize vLLM generator
    gen_fn = build_generator(args)
    print(f"Starting batched inference with vLLM (batch size: {args.batch_size})...")

    with open(args.output_file, "w", encoding="utf-8") as fout:
        for i in tqdm(range(0, len(prompts), args.batch_size), desc="Generating"):
            batch_prompts = prompts[i:i + args.batch_size]
            batch_outputs = gen_fn(batch_prompts)

            for output, prompt in zip(batch_outputs, batch_prompts):
                generation = output["generated_text"].replace(prompt, "").strip()
                fout.write(json.dumps([generation], ensure_ascii=False) + "\n")
            fout.flush()

            # Report memory usage for monitoring
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                used_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)
                print(f"Max GPU memory used: {used_memory:.2f} MB")

    print("vLLM inference completed successfully.")

if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %pip install sacrebleu>=2.3

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
eval_chrf.py  –  Compute ChrF for WAT JSONL outputs.

Example
-------
# print to console only
python eval_chrf.py refs.jsonl hyps.jsonl

# also append to a TSV file
python eval_chrf.py refs.jsonl hyps.jsonl --out scores.tsv
"""

from __future__ import annotations
import argparse, json, sys
from pathlib import Path
from typing import List

try:
    from sacrebleu.metrics import CHRF
except ImportError:
    sys.exit("Please `pip install sacrebleu>=2.3` first.")

# ---------------------------------------------------------------------------
def _extract(line: str) -> str:
    obj = json.loads(line) if line.strip().startswith(("[", "{", "\"")) else [line]
    return obj[0] if isinstance(obj, list) else obj.get("translation", "")

def _load(path: Path) -> List[str]:
    with path.open(encoding="utf-8") as f:
        return [_extract(ln).strip() for ln in f if ln.strip()]

# ---------------------------------------------------------------------------
def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--ref", help="Reference JSONL")
    ap.add_argument("--hyp", help="System output JSONL")
    ap.add_argument("--out", help="Optional file to append the score (TSV)")
    args = ap.parse_args()

    refs = _load(Path(args.ref))
    hyps = _load(Path(args.hyp))
    if len(refs) != len(hyps):
        sys.exit(f"[ERROR] Length mismatch: refs={len(refs)} vs hyps={len(hyps)}")

    score = CHRF().corpus_score(hyps, [refs]).score
    result_line = f"{Path(args.hyp).name}\t{score:.4f}"

    print(result_line)

    # optional file output
    if args.out:
        out_path = Path(args.out)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with out_path.open("a", encoding="utf-8") as f:
            f.write(result_line + "\n")

# ---------------------------------------------------------------------------
if __name__ == "__main__":
    main()

#!/usr/bin/env python
# results_aggregator.py
import pandas as pd
from pathlib import Path
import argparse
import re

def aggregate_results(scores_dir: Path, output_file: Path):
    """Aggregate all scores from TSV files into a single Excel/CSV file."""
    scores_data = []

    # Find all TSV files in the scores directory
    for tsv_file in scores_dir.glob("*.tsv"):
        # Extract language pair from filename
        filename = tsv_file.stem
        lang_pair_match = re.search(r"([a-z]{3}_[a-z]{3})", filename)
        lang_pair = lang_pair_match.group(1) if lang_pair_match else "unknown"

        # Read the TSV file
        df = pd.read_csv(tsv_file, sep="\t", header=None, names=["file", "score"])

        # Add language pair information
        df["lang_pair"] = lang_pair
        scores_data.append(df)

    # Combine all data
    if scores_data:
        combined_df = pd.concat(scores_data, ignore_index=True)

        # Save to Excel and CSV
        combined_df.to_excel(output_file.with_suffix(".xlsx"), index=False)
        combined_df.to_csv(output_file.with_suffix(".csv"), index=False)

        print(f"✓ Results saved to {output_file.with_suffix('.xlsx')} and {output_file.with_suffix('.csv')}")
        return combined_df
    else:
        print("No score files found")
        return pd.DataFrame()

def main():
    parser = argparse.ArgumentParser(description="Aggregate evaluation results into Excel/CSV")
    parser.add_argument("--scores_dir", required=True, help="Directory containing score TSV files")
    parser.add_argument("--output_file", required=True, help="Output file path (without extension)")
    args = parser.parse_args()

    scores_dir = Path(args.scores_dir)
    output_file = Path(args.output_file)

    aggregate_results(scores_dir, output_file)

if __name__ == "__main__":
    main()
