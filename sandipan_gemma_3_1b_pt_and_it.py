# -*- coding: utf-8 -*-
"""Sandipan gemma 3-1b PT and IT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hvc_SXZLuARIrcpM7EDwencSPy9RAMCx
"""

import json #Download Data#
from pathlib import Path
from typing import List

from datasets import load_dataset
from tqdm.auto import tqdm

PAIRS: List[str] = [
    "eng_ben", "eng_guj", "eng_hin", "eng_kan", "eng_mal",
    "eng_mar", "eng_ori", "eng_pan", "eng_tam", "eng_tel", "eng_urd",
]

def save_jsonl(lines: List[str], path: Path) -> None:
    """Saves a list of strings to a JSONL file."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for ln in lines:
            json.dump([ln], f, ensure_ascii=False)
            f.write("\n")

def dump_pair(subset: str, pair: str, out_root: Path) -> None:
    """Downloads and saves a specific language pair from a subset."""
    src_lang, tgt_lang = pair.split("_")
    ds = load_dataset("ai4bharat/Pralekha", subset, split=f"{src_lang}_{tgt_lang}")
    print(f"  {subset}/{pair}: {len(ds):,} docs")
    save_jsonl(ds["src_txt"], out_root / subset / pair / f"doc.{src_lang}.jsonl")
    save_jsonl(ds["tgt_txt"], out_root / subset / pair / f"doc.{tgt_lang}.jsonl")

def main() -> None:
    """Main function to download data."""
    out_root = Path("/tmp/pralekha_data").expanduser()
    splits = ["dev", "test"]

    print("Language pairs fixed to:", ", ".join(PAIRS))
    print("Splits:", ", ".join(splits))

    for subset in splits:
        for pair in tqdm(PAIRS, desc=f"[{subset}]"):
            dump_pair(subset, pair, out_root)

    print("\n✓ All data has been written to", out_root.resolve())

if __name__ == "__main__":
    main()

#!/usr/bin/env python    #Gen_prompt for 1 lang pair#
# -*- coding: utf-8 -*-
"""
Generate few-shot prompts (English side 101-200 words only).
Output: doc.{src}_2_{tgt}.{k}.jsonl
"""
from __future__ import annotations
import argparse, json, sys
from pathlib import Path
from typing import List

LANG_LABELS = {
    "eng": "English",   "ben": "Bengali",   "guj": "Gujarati",
    "hin": "Hindi",     "kan": "Kannada",   "mal": "Malayalam",
    "mar": "Marathi",   "ori": "Odia",      "pan": "Punjabi",
    "tam": "Tamil",     "tel": "Telugu",    "urd": "Urdu",
}

# ───────────────────────── helper funcs ─────────────────────────
def _extract_text(line: str) -> str:
    obj = json.loads(line)
    if isinstance(obj, list):
        return str(obj[0])
    if isinstance(obj, str):
        return obj
    raise ValueError("Line must be string or single-element list.")

def _load_texts(path: Path) -> List[str]:
    with path.open(encoding="utf-8") as f:
        return [_extract_text(ln).strip() for ln in f if ln.strip()]

def _build_block(src, tgt, src_lbl, tgt_lbl):
    return f"{src_lbl}: {src}\n\n{tgt_lbl}: {tgt}"

def _build_final_block(src, src_lbl, tgt_lbl):
    return f"{src_lbl}: {src}\n\n{tgt_lbl}:"

def _default_out_path(test_path: Path, src: str, tgt: str, k: int) -> Path:
    return test_path.parent / f"doc.{src}_2_{tgt}.{k}.jsonl"

def _word_len(txt: str) -> int:
    return len(txt.split())

# ─────────────────────────── main ───────────────────────────────
def main() -> None:
    # Set default values for the required arguments
    test_file = "/tmp/pralekha_data/test/eng_ben/doc.eng.jsonl"
    example_src_file = "/tmp/pralekha_data/dev/eng_ben/doc.eng.jsonl"
    example_tgt_file = "/tmp/pralekha_data/dev/eng_ben/doc.ben.jsonl"
    few_shot = 3
    src_lang = "eng"
    tgt_lang = "ben"
    output_file = None
    src_label = None
    tgt_label = None

    if few_shot < 0:
        sys.exit("few_shot must be ≥ 0")

    src_lbl = src_label or LANG_LABELS.get(src_lang, src_lang)
    tgt_lbl = tgt_label or LANG_LABELS.get(tgt_lang, tgt_lang)

    test_path   = Path(test_file)
    ex_src_path = Path(example_src_file)
    ex_tgt_path = Path(example_tgt_file)

    out_path = (
        Path(output_file)
        if output_file
        else _default_out_path(test_path, src_lang, tgt_lang, few_shot)
    )
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # ---- load all example pairs ---------------------------------------------
    ex_src = _load_texts(ex_src_path)
    ex_tgt = _load_texts(ex_tgt_path)
    if len(ex_src) != len(ex_tgt):
        sys.exit("[!] Example source / target length mismatch")

    # ---- pick first k whose English side is 101-200 words --------------------
    selected = []
    for s, t in zip(ex_src, ex_tgt):
        eng_side = s if src_lang == "eng" else t if tgt_lang == "eng" else None
        if eng_side is None:
            # neither side is English – should not happen for this task
            continue
        wlen = _word_len(eng_side)
        if 100 < wlen <= 200:
            selected.append((s, t))
            if len(selected) == few_shot:
                break

    if len(selected) < few_shot:
        print(f"[!] Only found {len(selected)} examples satisfying 101-200 words (needed {few_shot})")

    example_blocks = [_build_block(s, t, src_lbl, tgt_lbl) for s, t in selected]

    # ---- build prompts -------------------------------------------------------
    with test_path.open(encoding="utf-8") as fin, out_path.open("w", encoding="utf-8") as fout:
        tests = 0
        for raw in fin:
            if not raw.strip():
                continue
            test_src = _extract_text(raw).strip()
            parts = example_blocks + [_build_final_block(test_src, src_lbl, tgt_lbl)]
            fout.write(json.dumps(["\n\n".join(parts)], ensure_ascii=False) + "\n")
            tests += 1

    print(f"✓ {out_path} written. examples={len(selected)}, tests={tests}")

if __name__ == "__main__":
    main()



#!/usr/bin/env python                    #Gen_prompt for all lang pairs#
# -*- coding: utf-8 -*-
"""
Generate few-shot prompts for ALL language pairs (English side 101-200 words only).
Output: doc.{src}_2_{tgt}.{k}.jsonl for each language pair
"""
from __future__ import annotations
import argparse, json, sys
from pathlib import Path
from typing import List

LANG_LABELS = {
    "eng": "English",   "ben": "Bengali",   "guj": "Gujarati",
    "hin": "Hindi",     "kan": "Kannada",   "mal": "Malayalam",
    "mar": "Marathi",   "ori": "Odia",      "pan": "Punjabi",
    "tam": "Tamil",     "tel": "Telugu",    "urd": "Urdu",
}

# All language pairs to process
LANGUAGE_PAIRS = [
    # English to Indic languages
    ("eng", "ben"), ("eng", "guj"), ("eng", "hin"), ("eng", "kan"),
    ("eng", "mal"), ("eng", "mar"), ("eng", "ori"), ("eng", "pan"),
    ("eng", "tam"), ("eng", "tel"), ("eng", "urd"),

    # Indic languages to English (reverse direction)
    ("ben", "eng"), ("guj", "eng"), ("hin", "eng"), ("kan", "eng"),
    ("mal", "eng"), ("mar", "eng"), ("ori", "eng"), ("pan", "eng"),
    ("tam", "eng"), ("tel", "eng"), ("urd", "eng")
]

# ───────────────────────── helper funcs ─────────────────────────
def _extract_text(line: str) -> str:
    obj = json.loads(line)
    if isinstance(obj, list):
        return str(obj[0])
    if isinstance(obj, str):
        return obj
    raise ValueError("Line must be string or single-element list.")

def _load_texts(path: Path) -> List[str]:
    with path.open(encoding="utf-8") as f:
        return [_extract_text(ln).strip() for ln in f if ln.strip()]

def _build_block(src, tgt, src_lbl, tgt_lbl):
    return f"{src_lbl}: {src}\n\n{tgt_lbl}: {tgt}"

def _build_final_block(src, src_lbl, tgt_lbl):
    return f"{src_lbl}: {src}\n\n{tgt_lbl}:"

def _default_out_path(test_path: Path, src: str, tgt: str, k: int) -> Path:
    return test_path.parent / f"doc.{src}_2_{tgt}.{k}.jsonl"

def _word_len(txt: str) -> int:
    return len(txt.split())

def generate_prompts_for_pair(src_lang: str, tgt_lang: str, few_shot: int = 3, subset: str = "dev"):
    """Generate prompts for a specific language pair"""

    src_lbl = LANG_LABELS.get(src_lang, src_lang)
    tgt_lbl = LANG_LABELS.get(tgt_lang, tgt_lang)

    # Construct file paths for this language pair
    base_dir = "/tmp/pralekha_data"
    test_file = f"{base_dir}/test/{src_lang}_{tgt_lang}/doc.{src_lang}.jsonl"
    example_src_file = f"{base_dir}/{subset}/{src_lang}_{tgt_lang}/doc.{src_lang}.jsonl"
    example_tgt_file = f"{base_dir}/{subset}/{src_lang}_{tgt_lang}/doc.{tgt_lang}.jsonl"

    test_path = Path(test_file)
    ex_src_path = Path(example_src_file)
    ex_tgt_path = Path(example_tgt_file)

    # Check if files exist
    if not test_path.exists():
        print(f"⚠ Test file not found: {test_file}")
        return
    if not ex_src_path.exists() or not ex_tgt_path.exists():
        print(f"⚠ Example files not found for {src_lang}_{tgt_lang}")
        return

    out_path = _default_out_path(test_path, src_lang, tgt_lang, few_shot)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # Load all example pairs
    ex_src = _load_texts(ex_src_path)
    ex_tgt = _load_texts(ex_tgt_path)
    if len(ex_src) != len(ex_tgt):
        print(f"[!] Example source/target length mismatch for {src_lang}_{tgt_lang}")
        return

    # Pick first k examples where English side is 101-200 words
    selected = []
    for s, t in zip(ex_src, ex_tgt):
        # For English→Indic: English is source, for Indic→English: English is target
        eng_side = s if src_lang == "eng" else t if tgt_lang == "eng" else None
        if eng_side is None:
            continue
        wlen = _word_len(eng_side)
        if 100 < wlen <= 200:
            selected.append((s, t))
            if len(selected) == few_shot:
                break

    if len(selected) < few_shot:
        print(f"[!] Only found {len(selected)} examples satisfying 101-200 words for {src_lang}_{tgt_lang} (needed {few_shot})")

    example_blocks = [_build_block(s, t, src_lbl, tgt_lbl) for s, t in selected]

    # Build prompts
    with test_path.open(encoding="utf-8") as fin, out_path.open("w", encoding="utf-8") as fout:
        tests = 0
        for raw in fin:
            if not raw.strip():
                continue
            test_src = _extract_text(raw).strip()
            parts = example_blocks + [_build_final_block(test_src, src_lbl, tgt_lbl)]
            fout.write(json.dumps(["\n\n".join(parts)], ensure_ascii=False) + "\n")
            tests += 1

    print(f"✓ {out_path} written. examples={len(selected)}, tests={tests}")

# ─────────────────────────── main ───────────────────────────────
def main() -> None:
    few_shot = 3
    subset = "dev"  # Use "dev" set for few-shot examples

    print(f"Generating prompts for ALL language pairs (few_shot={few_shot})...")

    # Process all language pairs
    for src_lang, tgt_lang in LANGUAGE_PAIRS:
        print(f"\n{'='*50}")
        print(f"Processing: {LANG_LABELS[src_lang]} → {LANG_LABELS[tgt_lang]}")
        print(f"{'='*50}")

        generate_prompts_for_pair(src_lang, tgt_lang, few_shot, subset)

    print(f"\n{'='*50}")
    print("✓ Prompt generation completed for ALL language pairs!")
    print(f"{'='*50}")

if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %pip install vllm



from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')  # Set this in Colab secrets

#!/usr/bin/env python       #Translation _prompt for all lang pairs#
"""
Modified for Gemma 3-1B models with support for all language pairs
"""

import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
from pathlib import Path
from typing import List, Tuple
import re

from datasets import load_dataset
from vllm import LLM, SamplingParams

import os, re, subprocess, sys

# Add authentication at the very beginning
def setup_huggingface_auth():
    """Set up HuggingFace authentication"""
    # Try to get token from environment variable
    hf_token = os.environ.get("HF_TOKEN", "")

    if not hf_token:
        # Try to get from Colab secrets
        try:
            from google.colab import userdata
            hf_token = userdata.get('HF_TOKEN')
        except:
            pass

    if hf_token:
        os.environ["HUGGING_FACE_HUB_TOKEN"] = hf_token
        print("✓ HuggingFace token set from environment")
    else:
        print("⚠ No HF_TOKEN found. You may need to run: huggingface-cli login")

setup_huggingface_auth()

def _fix_cuda_visible_devices():
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "")
    if not cvd.startswith("GPU-"):
        # Already numeric -> nothing to do
        return

    # 1) Build a map {UUID -> index} from `nvidia-smi -L`
    try:
        smi = subprocess.check_output(["nvidia-smi", "-L"], text=True)
    except FileNotFoundError:
        sys.exit("!! nvidia-smi not found—cannot map GPU UUIDs to indices")

    uuid2idx = {}
    for line in smi.splitlines():
        # Example: "GPU 2: NVIDIA H100 (UUID: GPU-b1b8e0d1-5d40-8a62-d5da-393bca3fd881)"
        m = re.match(r"GPU\s+(\d+):.*\(UUID:\s+(GPU-[0-9a-f\-]+)\)", line)
        if m:
            idx, uuid = m.groups()
            uuid2idx[uuid] = idx

    # 2) Translate the job-allocated UUID list to indices
    try:
        new_ids = ",".join(uuid2idx[uuid] for uuid in cvd.split(","))
    except KeyError as e:
        missing = str(e).strip("'")
        sys.exit(f"!! UUID {missing} not found in `nvidia-smi -L` output.\n"
                 f"   CUDA_VISIBLE_DEVICES was: {cvd}")

    os.environ["CUDA_VISIBLE_DEVICES"] = new_ids
    print(f"[fix-gpu] CUDA_VISIBLE_DEVICES  {cvd}  →  {new_ids}")

_fix_cuda_visible_devices()

# ---------------------------------------------------------------------------
# 1. DATA LOADING (UNCHANGED)
# ---------------------------------------------------------------------------
def load_pralekha_split(
    src_lang: str = "eng",
    tgt_lang: str = "hin",
    subset: str = "dev",
    max_rows: int | None = None,  # Changed to process all documents
):
    ds = load_dataset("ai4bharat/Pralekha", f"{subset}", split=f"{src_lang}_{tgt_lang}")
    if max_rows:
        ds = ds.select(range(min(max_rows, len(ds))))
    return ds

# ---------------------------------------------------------------------------
# 2. TEXT PROCESSING FUNCTIONS
# ---------------------------------------------------------------------------
def split_into_sentences(text: str) -> List[str]:
    """Split text into sentences using simple regex"""
    # Basic sentence splitting - can be enhanced with NLTK if needed
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
    return [s.strip() for s in sentences if s.strip()]

def chunk_document(text: str, max_chars: int = 2000) -> List[str]:
    """Split document into manageable chunks"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        if current_length + len(word) + 1 > max_chars and current_chunk:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0

        current_chunk.append(word)
        current_length += len(word) + 1

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# ---------------------------------------------------------------------------
# 3. PROMPT TEMPLATING (Gemma 3 format for sentence-level translation)
# ---------------------------------------------------------------------------
_SYSTEM = (
    "You are a professional translator. Translate the following text precisely "
    "from {src} to {tgt}, preserving meaning, tone, and formatting. "
    "Return only the translation without any additional text."
)

def make_sentence_prompts(sentences: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for individual sentence translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {s}<end_of_turn>\n<start_of_turn>model\n"
        for s in sentences
    ]

def make_chunk_prompts(chunks: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for document chunk translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {chunk}<end_of_turn>\n<start_of_turn>model\n"
        for chunk in chunks
    ]

# ---------------------------------------------------------------------------
# 4. MODEL INSTANTIATION (Gemma 3-1B with auth handling)
# ---------------------------------------------------------------------------
def init_gemma(model_type: str = "it") -> LLM:
    """
    Load Gemma 3-1B model with authentication
    model_type: "pt" for pretrained, "it" for instruction-tuned
    """
    checkpoint = f"google/gemma-3-1b-{model_type}" if model_type == "it" else "google/gemma-3-1b"

    # Check if we have authentication
    if not os.environ.get("HUGGING_FACE_HUB_TOKEN"):
        print("⚠ Warning: No HuggingFace token found. You may need to:")
        print("  1. Run: huggingface-cli login")
        print("  2. Or set HF_TOKEN environment variable")
        print("  3. Accept model terms at: https://huggingface.co/google/gemma-3-1b-it")

    try:
        return LLM(
            model=checkpoint,
            dtype="float16",  # Changed from bfloat16 to float16
            tokenizer=checkpoint,
            max_model_len=4096,
            gpu_memory_utilization=0.6,  # Reduced gpu_memory_utilization
            trust_remote_code=True,
        )
    except Exception as e:
        if "gated" in str(e).lower() or "401" in str(e):
            print("\n❌ Authentication failed! You need to:")
            print("  1. Accept the model terms at: https://huggingface.co/google/gemma-3-1b-it")
            print("  2. Get a token from: https://huggingface.co/settings/tokens")
            print("  3. Either:")
            print("   - Run: huggingface-cli login")
            print("   - Or set HF_TOKEN environment variable")
        raise e

# ---------------------------------------------------------------------------
# 5. BATCH TRANSLATION
# ---------------------------------------------------------------------------
def translate(
    llm: LLM,
    prompts: List[str],
    temperature: float = 0.0,
    max_tokens: int = 512,  # Reduced for sentence-level translation
) -> List[str]:
    params = SamplingParams(temperature=temperature, max_tokens=max_tokens)
    outputs = llm.generate(prompts, params)
    return [out.outputs[0].text.strip() for out in outputs]

# ---------------------------------------------------------------------------
# 6. DOCUMENT TRANSLATION PIPELINE
# ---------------------------------------------------------------------------
def translate_document(
    llm: LLM,
    document: str,
    src_lang: str,
    tgt_lang: str,
    strategy: str = "sentence"  # "sentence" or "chunk"
) -> str:
    """
    Translate a document using either sentence-level or chunk-level approach
    """
    if strategy == "sentence":
        # Split into sentences and translate individually
        sentences = split_into_sentences(document)
        print(f"  Split into {len(sentences)} sentences")

        prompts = make_sentence_prompts(sentences, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=256)

        # Reconstruct document
        return " ".join(translations)

    else:  # chunk strategy
        # Split into manageable chunks
        chunks = chunk_document(document, max_chars=1500)
        print(f"  Split into {len(chunks)} chunks")

        prompts = make_chunk_prompts(chunks, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=1024)

        # Reconstruct document
        return " ".join(translations)

# ---------------------------------------------------------------------------
# 7. LANGUAGE PAIRS CONFIGURATION
# ---------------------------------------------------------------------------
# All supported language pairs
LANGUAGE_PAIRS = [
    ("eng", "ben"),  # English to Bengali
    ("eng", "guj"),  # English to Gujarati
    ("eng", "hin"),  # English to Hindi
    ("eng", "kan"),  # English to Kannada
    ("eng", "mal"),  # English to Malayalam
    ("eng", "mar"),  # English to Marathi
    ("eng", "ori"),  # English to Odia
    ("eng", "pan"),  # English to Punjabi
    ("eng", "tam"),  # English to Tamil
    ("eng", "tel"),  # English to Telugu
    ("eng", "urd"),  # English to Urdu

    # Reverse directions (Indic to English)
    ("ben", "eng"),  # Bengali to English
    ("guj", "eng"),  # Gujarati to English
    ("hin", "eng"),  # Hindi to English
    ("kan", "eng"),  # Kannada to English
    ("mal", "eng"),  # Malayalam to English
    ("mar", "eng"),  # Marathi to English
    ("ori", "eng"),  # Odia to English
    ("pan", "eng"),  # Punjabi to English
    ("tam", "eng"),  # Tamil to English
    ("tel", "eng"),  # Telugu to English
    ("urd", "eng"),  # Urdu to English
]

# Language code to name mapping for better prompts
LANG_NAMES = {
    "eng": "English",
    "ben": "Bengali",
    "guj": "Gujarati",
    "hin": "Hindi",
    "kan": "Kannada",
    "mal": "Malayalam",
    "mar": "Marathi",
    "ori": "Odia",
    "pan": "Punjabi",
    "tam": "Tamil",
    "tel": "Telugu",
    "urd": "Urdu",
}

# ---------------------------------------------------------------------------
# 8. BATCH PROCESSING FOR ALL LANGUAGES
# ---------------------------------------------------------------------------
def process_all_languages(
    subset: str = "dev",
    model_type: str = "it",
    strategy: str = "sentence",
    max_docs_per_lang: int = None  # Process all documents if None
):
    """Process all language pairs in both directions"""

    print(f"Initializing Gemma 3-1B-{model_type.upper()} model...")
    llm = init_gemma(model_type)
    print("✓ Model loaded successfully")

    for src_lang, tgt_lang in LANGUAGE_PAIRS:
        print(f"\n{'='*60}")
        print(f"Processing: {LANG_NAMES[src_lang]} → {LANG_NAMES[tgt_lang]}")
        print(f"{'='*60}")

        try:
            # Load dataset
            ds = load_pralekha_split(src_lang, tgt_lang, subset, max_docs_per_lang)
            print(f"Loaded {len(ds):,} documents from {subset}/{src_lang}_{tgt_lang}")

            if len(ds) == 0:
                print("⚠ No documents found, skipping...")
                continue

            # Translate all documents
            translations = []
            for i, doc in enumerate(ds["src_txt"]):
                print(f"Translating document {i+1}/{len(ds['src_txt'])}...")
                translated_doc = translate_document(llm, doc, LANG_NAMES[src_lang], LANG_NAMES[tgt_lang], strategy)
                translations.append(translated_doc)

                # Print progress every 10 documents
                if (i + 1) % 10 == 0:
                    print(f"  ✓ Completed {i + 1}/{len(ds['src_txt'])} documents")

            # Save results
            ds = ds.add_column("pred_txt", translations)
            out_file = Path(f"gemma3_1b_{model_type}_{strategy}_{src_lang}_to_{tgt_lang}_{subset}.csv")
            ds.to_pandas().to_csv(out_file, index=False)
            print(f"✓ Saved translations to {out_file.resolve()}")

        except Exception as e:
            print(f"❌ Error processing {src_lang}→{tgt_lang}: {e}")
            print("Skipping to next language pair...")
            continue

# ---------------------------------------------------------------------------
# 9. END-TO-END EXECUTION
# ---------------------------------------------------------------------------
def main():
    SUBSET = "dev"           # "dev", "test", or "train"
    MODEL_TYPE = "it"        # "pt" or "it"
    STRATEGY = "sentence"    # "sentence" or "chunk"
    MAX_DOCS = None          # Process all documents (set to number for testing)

    print("Starting translation for all language pairs...")
    print(f"Model: Gemma 3-1B-{MODEL_TYPE.upper()}")
    print(f"Strategy: {STRATEGY}-level translation")
    print(f"Dataset: {SUBSET} split")
    print(f"Max documents per language: {'All' if MAX_DOCS is None else MAX_DOCS}")

    process_all_languages(SUBSET, MODEL_TYPE, STRATEGY, MAX_DOCS)

    print("\n" + "="*60)
    print("✓ Translation completed for all language pairs!")
    print("="*60)

if __name__ == "__main__":
    main()

#!/usr/bin/env python  #Translate_prompt for N=10#
"""
Modified for Gemma 3-1B models with document chunking and sentence-level translation
"""

import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
from pathlib import Path
from typing import List, Tuple
import re

from datasets import load_dataset
from vllm import LLM, SamplingParams

import os, re, subprocess, sys

# Add authentication at the very beginning
def setup_huggingface_auth():
    """Set up HuggingFace authentication"""
    # Try to get token from environment variable
    hf_token = os.environ.get("HF_TOKEN", "")

    if not hf_token:
        # Try to get from Colab secrets
        try:
            from google.colab import userdata
            hf_token = userdata.get('HF_TOKEN')
        except:
            pass

    if hf_token:
        os.environ["HUGGING_FACE_HUB_TOKEN"] = hf_token
        print("✓ HuggingFace token set from environment")
    else:
        print("⚠ No HF_TOKEN found. You may need to run: huggingface-cli login")

setup_huggingface_auth()

def _fix_cuda_visible_devices():
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "")
    if not cvd.startswith("GPU-"):
        # Already numeric -> nothing to do
        return

    # 1) Build a map {UUID -> index} from `nvidia-smi -L`
    try:
        smi = subprocess.check_output(["nvidia-smi", "-L"], text=True)
    except FileNotFoundError:
        sys.exit("!! nvidia-smi not found—cannot map GPU UUIDs to indices")

    uuid2idx = {}
    for line in smi.splitlines():
        # Example: "GPU 2: NVIDIA H100 (UUID: GPU-b1b8e0d1-5d40-8a62-d5da-393bca3fd881)"
        m = re.match(r"GPU\s+(\d+):.*\(UUID:\s+(GPU-[0-9a-f\-]+)\)", line)
        if m:
            idx, uuid = m.groups()
            uuid2idx[uuid] = idx

    # 2) Translate the job-allocated UUID list to indices
    try:
        new_ids = ",".join(uuid2idx[uuid] for uuid in cvd.split(","))
    except KeyError as e:
        missing = str(e).strip("'")
        sys.exit(f"!! UUID {missing} not found in `nvidia-smi -L` output.\n"
                 f"   CUDA_VISIBLE_DEVICES was: {cvd}")

    os.environ["CUDA_VISIBLE_DEVICES"] = new_ids
    print(f"[fix-gpu] CUDA_VISIBLE_DEVICES  {cvd}  →  {new_ids}")

_fix_cuda_visible_devices()

# ---------------------------------------------------------------------------
# 1. DATA LOADING (UNCHANGED)
# ---------------------------------------------------------------------------
def load_pralekha_split(
    src_lang: str = "eng",
    tgt_lang: str = "hin",
    subset: str = "dev",
    max_rows: int | None = 100,
):
    ds = load_dataset("ai4bharat/Pralekha", f"{subset}", split=f"{src_lang}_{tgt_lang}")
    if max_rows:
        ds = ds.select(range(min(max_rows, len(ds))))
    return ds

# ---------------------------------------------------------------------------
# 2. TEXT PROCESSING FUNCTIONS
# ---------------------------------------------------------------------------
def split_into_sentences(text: str) -> List[str]:
    """Split text into sentences using simple regex"""
    # Basic sentence splitting - can be enhanced with NLTK if needed
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
    return [s.strip() for s in sentences if s.strip()]

def chunk_document(text: str, max_chars: int = 2000) -> List[str]:
    """Split document into manageable chunks"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        if current_length + len(word) + 1 > max_chars and current_chunk:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0

        current_chunk.append(word)
        current_length += len(word) + 1

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# ---------------------------------------------------------------------------
# 3. PROMPT TEMPLATING (Gemma 3 format for sentence-level translation)
# ---------------------------------------------------------------------------
_SYSTEM = (
    "You are a professional translator. Translate the following text precisely "
    "from {src} to {tgt}, preserving meaning, tone, and formatting. "
    "Return only the translation without any additional text."
)

def make_sentence_prompts(sentences: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for individual sentence translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {s}<end_of_turn>\n<start_of_turn>model\n"
        for s in sentences
    ]

def make_chunk_prompts(chunks: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for document chunk translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {chunk}<end_of_turn>\n<start_of_turn>model\n"
        for chunk in chunks
    ]

# ---------------------------------------------------------------------------
# 4. MODEL INSTANTIATION (Gemma 3-1B with auth handling)
# ---------------------------------------------------------------------------
def init_gemma(model_type: str = "it") -> LLM:
    """
    Load Gemma 3-1B model with authentication
    model_type: "pt" for pretrained, "it" for instruction-tuned
    """
    checkpoint = f"google/gemma-3-1b-{model_type}" if model_type == "it" else "google/gemma-3-1b"

    # Check if we have authentication
    if not os.environ.get("HUGGING_FACE_HUB_TOKEN"):
        print("⚠ Warning: No HuggingFace token found. You may need to:")
        print("  1. Run: huggingface-cli login")
        print("  2. Or set HF_TOKEN environment variable")
        print("  3. Accept model terms at: https://huggingface.co/google/gemma-3-1b-it")

    try:
        return LLM(
            model=checkpoint,
            dtype="float16", # Changed from bfloat16 to float16
            tokenizer=checkpoint,
            max_model_len=4096,
            gpu_memory_utilization=0.8,
            trust_remote_code=True,
        )
    except Exception as e:
        if "gated" in str(e).lower() or "401" in str(e):
            print("\n❌ Authentication failed! You need to:")
            print("  1. Accept the model terms at: https://huggingface.co/google/gemma-3-1b-it")
            print("  2. Get a token from: https://huggingface.co/settings/tokens")
            print("  3. Either:")
            print("   - Run: huggingface-cli login")
            print("   - Or set HF_TOKEN environment variable")
        raise e

# ---------------------------------------------------------------------------
# 5. BATCH TRANSLATION
# ---------------------------------------------------------------------------
def translate(
    llm: LLM,
    prompts: List[str],
    temperature: float = 0.0,
    max_tokens: int = 512,  # Reduced for sentence-level translation
) -> List[str]:
    params = SamplingParams(temperature=temperature, max_tokens=max_tokens)
    outputs = llm.generate(prompts, params)
    return [out.outputs[0].text.strip() for out in outputs]

# ---------------------------------------------------------------------------
# 6. DOCUMENT TRANSLATION PIPELINE
# ---------------------------------------------------------------------------
def translate_document(
    llm: LLM,
    document: str,
    src_lang: str,
    tgt_lang: str,
    strategy: str = "sentence"  # "sentence" or "chunk"
) -> str:
    """
    Translate a document using either sentence-level or chunk-level approach
    """
    if strategy == "sentence":
        # Split into sentences and translate individually
        sentences = split_into_sentences(document)
        print(f"  Split into {len(sentences)} sentences")

        prompts = make_sentence_prompts(sentences, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=256)

        # Reconstruct document
        return " ".join(translations)

    else:  # chunk strategy
        # Split into manageable chunks
        chunks = chunk_document(document, max_chars=1500)
        print(f"  Split into {len(chunks)} chunks")

        prompts = make_chunk_prompts(chunks, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=1024)

        # Reconstruct document
        return " ".join(translations)

# ---------------------------------------------------------------------------
# 7. END-TO-END EXECUTION
# ---------------------------------------------------------------------------
def main():
    SRC, TGT = "eng", "hin"
    SUBSET = "dev"
    N = 10  # Reduced for testing - increase after it works
    MODEL_TYPE = "it"
    STRATEGY = "sentence"  # "sentence" or "chunk"

    print(f"Loading {MODEL_TYPE} model for {SRC}->{TGT}...")
    print(f"Using {STRATEGY}-level translation strategy")

    ds = load_pralekha_split(SRC, TGT, SUBSET, N)
    print(f"Loaded {len(ds):,} rows from {SUBSET}/{SRC}_{TGT}")

    try:
        llm = init_gemma(MODEL_TYPE)
        print("✓ Model loaded successfully")

        translations = []
        for i, doc in enumerate(ds["src_txt"]):
            print(f"Translating document {i+1}/{len(ds['src_txt'])}...")
            translated_doc = translate_document(llm, doc, SRC, TGT, STRATEGY)
            translations.append(translated_doc)
            print(f"  ✓ Completed document {i+1}")


        # Add predictions & persist
        ds = ds.add_column("pred_txt", translations)
        out_file = Path(f"gemma3_1b_{MODEL_TYPE}_{STRATEGY}_translations_{SRC}_{TGT}_{SUBSET}_{N}.csv")
        ds.to_pandas().to_csv(out_file, index=False)
        print(f"✓ Saved translations to {out_file.resolve()}")

    except Exception as e:
        print(f"❌ Error: {e}")
        print("\nMake sure you:")
        print("1. Accepted the model terms: https://huggingface.co/google/gemma-3-1b-it")
        print("2. Have a valid HF token: https://huggingface.co/settings/tokens")
        print("3. Either:")
        print("   - Run: huggingface-cli login")
        print("   - Or set HF_TOKEN environment variable")


if __name__ == "__main__":
    main()

#!/usr/bin/env python #inference prompt NOT for all lang pairs#
"""
Modified for Gemma 3-1B models with document chunking and sentence-level translation
"""

import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
from pathlib import Path
from typing import List, Tuple
import re

from datasets import load_dataset
from vllm import LLM, SamplingParams

import os, re, subprocess, sys

# Add authentication at the very beginning
def setup_huggingface_auth():
    """Set up HuggingFace authentication"""
    # Try to get token from environment variable
    hf_token = os.environ.get("HF_TOKEN", "")

    if not hf_token:
        # Try to get from Colab secrets
        try:
            from google.colab import userdata
            hf_token = userdata.get('HF_TOKEN')
        except:
            pass

    if hf_token:
        os.environ["HUGGING_FACE_HUB_TOKEN"] = hf_token
        print("✓ HuggingFace token set from environment")
    else:
        print("⚠ No HF_TOKEN found. You may need to run: huggingface-cli login")

setup_huggingface_auth()

def _fix_cuda_visible_devices():
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "")
    if not cvd.startswith("GPU-"):
        # Already numeric -> nothing to do
        return

    # 1) Build a map {UUID -> index} from `nvidia-smi -L`
    try:
        smi = subprocess.check_output(["nvidia-smi", "-L"], text=True)
    except FileNotFoundError:
        sys.exit("!! nvidia-smi not found—cannot map GPU UUIDs to indices")

    uuid2idx = {}
    for line in smi.splitlines():
        # Example: "GPU 2: NVIDIA H100 (UUID: GPU-b1b8e0d1-5d40-8a62-d5da-393bca3fd881)"
        m = re.match(r"GPU\s+(\d+):.*\(UUID:\s+(GPU-[0-9a-f\-]+)\)", line)
        if m:
            idx, uuid = m.groups()
            uuid2idx[uuid] = idx

    # 2) Translate the job-allocated UUID list to indices
    try:
        new_ids = ",".join(uuid2idx[uuid] for uuid in cvd.split(","))
    except KeyError as e:
        missing = str(e).strip("'")
        sys.exit(f"!! UUID {missing} not found in `nvidia-smi -L` output.\n"
                 f"   CUDA_VISIBLE_DEVICES was: {cvd}")

    os.environ["CUDA_VISIBLE_DEVICES"] = new_ids
    print(f"[fix-gpu] CUDA_VISIBLE_DEVICES  {cvd}  →  {new_ids}")

_fix_cuda_visible_devices()

# ---------------------------------------------------------------------------
# 1. DATA LOADING (UNCHANGED)
# ---------------------------------------------------------------------------
def load_pralekha_split(
    src_lang: str = "eng",
    tgt_lang: str = "hin",
    subset: str = "dev",
    max_rows: int | None = 100,
):
    ds = load_dataset("ai4bharat/Pralekha", f"{subset}", split=f"{src_lang}_{tgt_lang}")
    if max_rows:
        ds = ds.select(range(min(max_rows, len(ds))))
    return ds

# ---------------------------------------------------------------------------
# 2. TEXT PROCESSING FUNCTIONS
# ---------------------------------------------------------------------------
def split_into_sentences(text: str) -> List[str]:
    """Split text into sentences using simple regex"""
    # Basic sentence splitting - can be enhanced with NLTK if needed
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
    return [s.strip() for s in sentences if s.strip()]

def chunk_document(text: str, max_chars: int = 2000) -> List[str]:
    """Split document into manageable chunks"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        if current_length + len(word) + 1 > max_chars and current_chunk:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0

        current_chunk.append(word)
        current_length += len(word) + 1

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# ---------------------------------------------------------------------------
# 3. PROMPT TEMPLATING (Gemma 3 format for sentence-level translation)
# ---------------------------------------------------------------------------
_SYSTEM = (
    "You are a professional translator. Translate the following text precisely "
    "from {src} to {tgt}, preserving meaning, tone, and formatting. "
    "Return only the translation without any additional text."
)

def make_sentence_prompts(sentences: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for individual sentence translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {s}<end_of_turn>\n<start_of_turn>model\n"
        for s in sentences
    ]

def make_chunk_prompts(chunks: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for document chunk translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {chunk}<end_of_turn>\n<start_of_turn>model\n"
        for chunk in chunks
    ]

# ---------------------------------------------------------------------------
# 4. MODEL INSTANTIATION (Gemma 3-1B with auth handling)
# ---------------------------------------------------------------------------
def init_gemma(model_type: str = "it") -> LLM:
    """
    Load Gemma 3-1B model with authentication
    model_type: "pt" for pretrained, "it" for instruction-tuned
    """
    checkpoint = f"google/gemma-3-1b-{model_type}" if model_type == "it" else "google/gemma-3-1b"

    # Check if we have authentication
    if not os.environ.get("HUGGING_FACE_HUB_TOKEN"):
        print("⚠ Warning: No HuggingFace token found. You may need to:")
        print("  1. Run: huggingface-cli login")
        print("  2. Or set HF_TOKEN environment variable")
        print("  3. Accept model terms at: https://huggingface.co/google/gemma-3-1b-it")

    try:
        return LLM(
            model=checkpoint,
            dtype="float16", # Changed from bfloat16 to float16
            tokenizer=checkpoint,
            max_model_len=4096,
            gpu_memory_utilization=0.6, # Reduced gpu_memory_utilization
            trust_remote_code=True,
        )
    except Exception as e:
        if "gated" in str(e).lower() or "401" in str(e):
            print("\n❌ Authentication failed! You need to:")
            print("  1. Accept the model terms at: https://huggingface.co/google/gemma-3-1b-it")
            print("  2. Get a token from: https://huggingface.co/settings/tokens")
            print("  3. Either:")
            print("   - Run: huggingface-cli login")
            print("   - Or set HF_TOKEN environment variable")
        raise e

# ---------------------------------------------------------------------------
# 5. BATCH TRANSLATION
# ---------------------------------------------------------------------------
def translate(
    llm: LLM,
    prompts: List[str],
    temperature: float = 0.0,
    max_tokens: int = 512,  # Reduced for sentence-level translation
) -> List[str]:
    params = SamplingParams(temperature=temperature, max_tokens=max_tokens)
    outputs = llm.generate(prompts, params)
    return [out.outputs[0].text.strip() for out in outputs]

# ---------------------------------------------------------------------------
# 6. DOCUMENT TRANSLATION PIPELINE
# ---------------------------------------------------------------------------
def translate_document(
    llm: LLM,
    document: str,
    src_lang: str,
    tgt_lang: str,
    strategy: str = "sentence"  # "sentence" or "chunk"
) -> str:
    """
    Translate a document using either sentence-level or chunk-level approach
    """
    if strategy == "sentence":
        # Split into sentences and translate individually
        sentences = split_into_sentences(document)
        print(f"  Split into {len(sentences)} sentences")

        prompts = make_sentence_prompts(sentences, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=256)

        # Reconstruct document
        return " ".join(translations)

    else:  # chunk strategy
        # Split into manageable chunks
        chunks = chunk_document(document, max_chars=1500)
        print(f"  Split into {len(chunks)} chunks")

        prompts = make_chunk_prompts(chunks, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=1024)

        # Reconstruct document
        return " ".join(translations)

# ---------------------------------------------------------------------------
# 7. END-TO-END EXECUTION
# ---------------------------------------------------------------------------
def main():
    SRC, TGT = "eng", "hin"
    SUBSET = "dev"
    N = 10  # Reduced for testing - increase after it works
    MODEL_TYPE = "it"
    STRATEGY = "sentence"  # "sentence" or "chunk"

    print(f"Loading {MODEL_TYPE} model for {SRC}->{TGT}...")
    print(f"Using {STRATEGY}-level translation strategy")

    ds = load_pralekha_split(SRC, TGT, SUBSET, N)
    print(f"Loaded {len(ds):,} rows from {SUBSET}/{SRC}_{TGT}")

    try:
        llm = init_gemma(MODEL_TYPE)
        print("✓ Model loaded successfully")

        translations = []
        for i, doc in enumerate(ds["src_txt"]):
            print(f"Translating document {i+1}/{len(ds['src_txt'])}...")
            translated_doc = translate_document(llm, doc, SRC, TGT, STRATEGY)
            translations.append(translated_doc)
            print(f"  ✓ Completed document {i+1}")


        # Add predictions & persist
        ds = ds.add_column("pred_txt", translations)
        out_file = Path(f"gemma3_1b_{MODEL_TYPE}_{STRATEGY}_translations_{SRC}_{TGT}_{SUBSET}_{N}.csv")
        ds.to_pandas().to_csv(out_file, index=False)
        print(f"✓ Saved translations to {out_file.resolve()}")

    except Exception as e:
        print(f"❌ Error: {e}")
        print("\nMake sure you:")
        print("1. Accepted the model terms: https://huggingface.co/google/gemma-3-1b-it")
        print("2. Have a valid HF token: https://huggingface.co/settings/tokens")
        print("3. Either:")
        print("   - Run: huggingface-cli login")
        print("   - Or set HF_TOKEN environment variable")


if __name__ == "__main__":
    main()

#!/usr/bin/env python
"""
Modified for Gemma 3-1B models with document chunking and sentence-level translation
FOR ALL LANGUAGES - FOR ALL PAIRS - INFERENCE CODE
"""

import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
from pathlib import Path
from typing import List, Tuple
import re

from datasets import load_dataset
from vllm import LLM, SamplingParams

import os, re, subprocess, sys

# Add authentication at the very beginning
def setup_huggingface_auth():
    """Set up HuggingFace authentication"""
    # Try to get token from environment variable
    hf_token = os.environ.get("HF_TOKEN", "")

    if not hf_token:
        # Try to get from Colab secrets
        try:
            from google.colab import userdata
            hf_token = userdata.get('HF_TOKEN')
        except:
            pass

    if hf_token:
        os.environ["HUGGING_FACE_HUB_TOKEN"] = hf_token
        print("✓ HuggingFace token set from environment")
    else:
        print("⚠ No HF_TOKEN found. You may need to run: huggingface-cli login")

setup_huggingface_auth()

def _fix_cuda_visible_devices():
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "")
    if not cvd.startswith("GPU-"):
        # Already numeric -> nothing to do
        return

    # 1) Build a map {UUID -> index} from `nvidia-smi -L`
    try:
        smi = subprocess.check_output(["nvidia-smi", "-L"], text=True)
    except FileNotFoundError:
        sys.exit("!! nvidia-smi not found—cannot map GPU UUIDs to indices")

    uuid2idx = {}
    for line in smi.splitlines():
        # Example: "GPU 2: NVIDIA H100 (UUID: GPU-b1b8e0d1-5d40-8a62-d5da-393bca3fd881)"
        m = re.match(r"GPU\s+(\d+):.*\(UUID:\s+(GPU-[0-9a-f\-]+)\)", line)
        if m:
            idx, uuid = m.groups()
            uuid2idx[uuid] = idx

    # 2) Translate the job-allocated UUID list to indices
    try:
        new_ids = ",".join(uuid2idx[uuid] for uuid in cvd.split(","))
    except KeyError as e:
        missing = str(e).strip("'")
        sys.exit(f"!! UUID {missing} not found in `nvidia-smi -L` output.\n"
                 f"   CUDA_VISIBLE_DEVICES was: {cvd}")

    os.environ["CUDA_VISIBLE_DEVICES"] = new_ids
    print(f"[fix-gpu] CUDA_VISIBLE_DEVICES  {cvd}  →  {new_ids}")

_fix_cuda_visible_devices()

# ---------------------------------------------------------------------------
# 1. DATA LOADING (UNCHANGED)
# ---------------------------------------------------------------------------
def load_pralekha_split(
    src_lang: str = "eng",
    tgt_lang: str = "hin",
    subset: str = "dev",
    max_rows: int | None = 100,
):
    ds = load_dataset("ai4bharat/Pralekha", f"{subset}", split=f"{src_lang}_{tgt_lang}")
    if max_rows:
        ds = ds.select(range(min(max_rows, len(ds))))
    return ds

# ---------------------------------------------------------------------------
# 2. TEXT PROCESSING FUNCTIONS
# ---------------------------------------------------------------------------
def split_into_sentences(text: str) -> List[str]:
    """Split text into sentences using simple regex"""
    # Basic sentence splitting - can be enhanced with NLTK if needed
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
    return [s.strip() for s in sentences if s.strip()]

def chunk_document(text: str, max_chars: int = 2000) -> List[str]:
    """Split document into manageable chunks"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        if current_length + len(word) + 1 > max_chars and current_chunk:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0

        current_chunk.append(word)
        current_length += len(word) + 1

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# ---------------------------------------------------------------------------
# 3. PROMPT TEMPLATING (Gemma 3 format for sentence-level translation)
# ---------------------------------------------------------------------------
_SYSTEM = (
    "You are a professional translator. Translate the following text precisely "
    "from {src} to {tgt}, preserving meaning, tone, and formatting. "
    "Return only the translation without any additional text."
)

def make_sentence_prompts(sentences: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for individual sentence translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {s}<end_of_turn>\n<start_of_turn>model\n"
        for s in sentences
    ]

def make_chunk_prompts(chunks: List[str], src: str, tgt: str) -> List[str]:
    """Create prompts for document chunk translation"""
    sys_msg = _SYSTEM.format(src=src, tgt=tgt)
    return [
        f"<start_of_turn>user\n{sys_msg}\n\nText to translate: {chunk}<end_of_turn>\n<start_of_turn>model\n"
        for chunk in chunks
    ]

# ---------------------------------------------------------------------------
# 4. MODEL INSTANTIATION (Gemma 3-1B with auth handling)
# ---------------------------------------------------------------------------
def init_gemma(model_type: str = "it") -> LLM:
    """
    Load Gemma 3-1B model with authentication
    model_type: "pt" for pretrained, "it" for instruction-tuned
    """
    checkpoint = f"google/gemma-3-1b-{model_type}" if model_type == "it" else "google/gemma-3-1b"

    # Check if we have authentication
    if not os.environ.get("HUGGING_FACE_HUB_TOKEN"):
        print("⚠ Warning: No HuggingFace token found. You may need to:")
        print("  1. Run: huggingface-cli login")
        print("  2. Or set HF_TOKEN environment variable")
        print("  3. Accept model terms at: https://huggingface.co/google/gemma-3-1b-it")

    try:
        return LLM(
            model=checkpoint,
            dtype="float16", # Changed from bfloat16 to float16
            tokenizer=checkpoint,
            max_model_len=4096,
            gpu_memory_utilization=0.6, # Reduced gpu_memory_utilization
            trust_remote_code=True,
        )
    except Exception as e:
        if "gated" in str(e).lower() or "401" in str(e):
            print("\n❌ Authentication failed! You need to:")
            print("  1. Accept the model terms at: https://huggingface.co/google/gemma-3-1b-it")
            print("  2. Get a token from: https://huggingface.co/settings/tokens")
            print("  3. Either:")
            print("   - Run: huggingface-cli login")
            print("   - Or set HF_TOKEN environment variable")
        raise e

# ---------------------------------------------------------------------------
# 5. BATCH TRANSLATION
# ---------------------------------------------------------------------------
def translate(
    llm: LLM,
    prompts: List[str],
    temperature: float = 0.0,
    max_tokens: int = 512,  # Reduced for sentence-level translation
) -> List[str]:
    params = SamplingParams(temperature=temperature, max_tokens=max_tokens)
    outputs = llm.generate(prompts, params)
    return [out.outputs[0].text.strip() for out in outputs]

# ---------------------------------------------------------------------------
# 6. DOCUMENT TRANSLATION PIPELINE
# ---------------------------------------------------------------------------
def translate_document(
    llm: LLM,
    document: str,
    src_lang: str,
    tgt_lang: str,
    strategy: str = "sentence"  # "sentence" or "chunk"
) -> str:
    """
    Translate a document using either sentence-level or chunk-level approach
    """
    if strategy == "sentence":
        # Split into sentences and translate individually
        sentences = split_into_sentences(document)
        print(f"  Split into {len(sentences)} sentences")

        prompts = make_sentence_prompts(sentences, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=256)

        # Reconstruct document
        return " ".join(translations)

    else:  # chunk strategy
        # Split into manageable chunks
        chunks = chunk_document(document, max_chars=1500)
        print(f"  Split into {len(chunks)} chunks")

        prompts = make_chunk_prompts(chunks, src_lang, tgt_lang)
        translations = translate(llm, prompts, max_tokens=1024)

        # Reconstruct document
        return " ".join(translations)

# ---------------------------------------------------------------------------
# 7. LANGUAGE CONFIGURATION - ALL SUPPORTED PAIRS
# ---------------------------------------------------------------------------
# All language pairs to process
LANGUAGE_PAIRS = [
    # English to Indic languages
    ("eng", "ben"),  # English to Bengali
    ("eng", "guj"),  # English to Gujarati
    ("eng", "hin"),  # English to Hindi
    ("eng", "kan"),  # English to Kannada
    ("eng", "mal"),  # English to Malayalam
    ("eng", "mar"),  # English to Marathi
    ("eng", "ori"),  # English to Odia
    ("eng", "pan"),  # English to Punjabi
    ("eng", "tam"),  # English to Tamil
    ("eng", "tel"),  # English to Telugu
    ("eng", "urd"),  # English to Urdu

    # Indic languages to English (reverse direction)
    ("ben", "eng"),  # Bengali to English
    ("guj", "eng"),  # Gujarati to English
    ("hin", "eng"),  # Hindi to English
    ("kan", "eng"),  # Kannada to English
    ("mal", "eng"),  # Malayalam to English
    ("mar", "eng"),  # Marathi to English
    ("ori", "eng"),  # Odia to English
    ("pan", "eng"),  # Punjabi to English
    ("tam", "eng"),  # Tamil to English
    ("tel", "eng"),  # Telugu to English
    ("urd", "eng"),  # Urdu to English
]

# Language code to name mapping for better prompts
LANG_NAMES = {
    "eng": "English",
    "ben": "Bengali",
    "guj": "Gujarati",
    "hin": "Hindi",
    "kan": "Kannada",
    "mal": "Malayalam",
    "mar": "Marathi",
    "ori": "Odia",
    "pan": "Punjabi",
    "tam": "Tamil",
    "tel": "Telugu",
    "urd": "Urdu",
}

# ---------------------------------------------------------------------------
# 8. PROCESS ALL LANGUAGES
# ---------------------------------------------------------------------------
def process_all_languages():
    """Process translation for all language pairs"""
    SUBSET = "dev"
    N = 10  # Reduced for testing - increase after it works
    MODEL_TYPE = "it"
    STRATEGY = "sentence"  # "sentence" or "chunk"

    print("Initializing Gemma model...")
    try:
        llm = init_gemma(MODEL_TYPE)
        print("✓ Model loaded successfully")
    except Exception as e:
        print(f"❌ Failed to load model: {e}")
        return

    # Process each language pair
    for src_lang, tgt_lang in LANGUAGE_PAIRS:
        print(f"\n{'='*60}")
        print(f"Processing: {LANG_NAMES[src_lang]} → {LANG_NAMES[tgt_lang]}")
        print(f"{'='*60}")

        try:
            # Load dataset for this language pair
            ds = load_pralekha_split(src_lang, tgt_lang, SUBSET, N)
            print(f"Loaded {len(ds):,} rows from {SUBSET}/{src_lang}_{tgt_lang}")

            if len(ds) == 0:
                print("⚠ No documents found, skipping...")
                continue

            translations = []
            for i, doc in enumerate(ds["src_txt"]):
                print(f"Translating document {i+1}/{len(ds['src_txt'])}...")
                translated_doc = translate_document(llm, doc, LANG_NAMES[src_lang], LANG_NAMES[tgt_lang], STRATEGY)
                translations.append(translated_doc)
                print(f"  ✓ Completed document {i+1}")

            # Add predictions & persist
            ds = ds.add_column("pred_txt", translations)
            out_file = Path(f"gemma3_1b_{MODEL_TYPE}_{STRATEGY}_{src_lang}_to_{tgt_lang}_{SUBSET}_{N}.csv")
            ds.to_pandas().to_csv(out_file, index=False)
            print(f"✓ Saved translations to {out_file.resolve()}")

        except Exception as e:
            print(f"❌ Error processing {src_lang}→{tgt_lang}: {e}")
            print("Skipping to next language pair...")
            continue

    print(f"\n{'='*60}")
    print("✓ Translation completed for ALL language pairs!")
    print(f"{'='*60}")

# ---------------------------------------------------------------------------
# 9. MAIN EXECUTION
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    process_all_languages()

# Commented out IPython magic to ensure Python compatibility.
# %pip install sacrebleu>=2.3

# Check current directory structure
!pwd
!ls -la

# Check if /content directory exists
!ls -la /content/

# Check if data directory exists
!ls -la /content/data/ 2>/dev/null || echo "No data directory found"

# Check if any JSONL files exist
!find /content -name "*.jsonl" 2>/dev/null || echo "No JSONL files found"

# Check if any CSV files exist (from your translations)
!find /content -name "*.csv" 2>/dev/null || echo "No CSV files found"

# Install required packages
!pip install datasets

# Download Pralekha data
from datasets import load_dataset
from pathlib import Path
import json

def download_pralekha_data():
    print("Downloading Pralekha dataset...")

    # Create data directory
    data_dir = Path("/content/data")
    data_dir.mkdir(parents=True, exist_ok=True)

    # Download English to Hindi dev set
    try:
        dataset = load_dataset("ai4bharat/Pralekha", "dev", split="eng_hin")
        print(f"Downloaded {len(dataset)} samples")

        # Save to JSONL files
        eng_file = data_dir / "dev" / "eng_hin" / "doc.eng.jsonl"
        hin_file = data_dir / "dev" / "eng_hin" / "doc.hin.jsonl"

        eng_file.parent.mkdir(parents=True, exist_ok=True)

        # Save English texts
        with open(eng_file, 'w', encoding='utf-8') as f:
            for item in dataset["src_txt"]:
                json.dump([item], f, ensure_ascii=False)
                f.write('\n')

        # Save Hindi texts
        with open(hin_file, 'w', encoding='utf-8') as f:
            for item in dataset["tgt_txt"]:
                json.dump([item], f, ensure_ascii=False)
                f.write('\n')

        print(f"Saved English data to: {eng_file}")
        print(f"Saved Hindi data to: {hin_file}")

        return eng_file, hin_file

    except Exception as e:
        print(f"Error downloading dataset: {e}")
        return None, None

# Download the data
ref_file, hyp_file = download_pralekha_data()

# Look for your translation output files
import glob

# Check for Gemma translation files
translation_files = glob.glob("/content/*gemma*.jsonl") + glob.glob("/content/*gemma*.csv")
print("Found translation files:", translation_files)

# If you find CSV files but need JSONL, convert them
if translation_files and not any(f.endswith('.jsonl') for f in translation_files):
    print("Converting CSV to JSONL...")
    import pandas as pd

    for csv_file in translation_files:
        if csv_file.endswith('.csv'):
            df = pd.read_csv(csv_file)
            jsonl_file = csv_file.replace('.csv', '.jsonl')

            with open(jsonl_file, 'w', encoding='utf-8') as f:
                for text in df['pred_txt']:
                    json.dump([text], f, ensure_ascii=False)
                    f.write('\n')

            print(f"Converted {csv_file} to {jsonl_file}")

# First install sacrebleu if not already installed
!pip install sacrebleu>=2.3.0

# Then run the evaluation with correct file paths
from pathlib import Path
import json
from sacrebleu.metrics import CHRF

def _extract(line: str) -> str:
    try:
        obj = json.loads(line) if line.strip().startswith(("[", "{", "\"")) else [line]
        return obj[0] if isinstance(obj, list) else obj.get("translation", "")
    except:
        return line.strip()

def _load(path: Path):
    texts = []
    try:
        with path.open(encoding="utf-8") as f:
            for ln in f:
                if ln.strip():
                    texts.append(_extract(ln))
        return texts
    except FileNotFoundError:
        print(f"File not found: {path}")
        return []

# Try these possible file locations
possible_ref_locations = [
    "/content/data/dev/eng_hin/doc.hin.jsonl",
    "/content/data/eng_hin/doc.hin.jsonl",
    "/content/doc.hin.jsonl",
    "/content/hindi_references.jsonl"
]

possible_hyp_locations = [
    "/content/gemma3_1b_it_translations_eng_hin_dev_10.jsonl",
    "/content/gemma3_1b_it_eng_to_hin_dev_10.jsonl",
    "/content/gemma_translations.jsonl",
    "/content/translations.jsonl"
]

# Find the actual files
ref_file = None
hyp_file = None

for location in possible_ref_locations:
    if Path(location).exists():
        ref_file = location
        print(f"Found reference file: {ref_file}")
        break

for location in possible_hyp_locations:
    if Path(location).exists():
        hyp_file = location
        print(f"Found hypothesis file: {hyp_file}")
        break

if not ref_file or not hyp_file:
    print("Could not find required files. Please check the file paths.")
    print("Available files:")
    !find /content -name "*.jsonl" -o -name "*.csv" 2>/dev/null
else:
    # Load files
    refs = _load(Path(ref_file))
    hyps = _load(Path(hyp_file))

    print(f"References: {len(refs)}, Hypotheses: {len(hyps)}")

    if len(refs) != len(hyps):
        print(f"Warning: Length mismatch! refs={len(refs)} vs hyps={len(hyps)}")
        # Use the minimum length to avoid errors
        min_len = min(len(refs), len(hyps))
        refs = refs[:min_len]
        hyps = hyps[:min_len]
        print(f"Using first {min_len} samples for evaluation")

    # Compute ChrF score
    if refs and hyps:
        score = CHRF().corpus_score(hyps, [refs]).score
        print(f"ChrF Score: {score:.4f}")

        # Save to file
        with open("/content/scores.tsv", "a") as f:
            f.write(f"{Path(hyp_file).name}\t{score:.4f}\n")
        print("Score saved to /content/scores.tsv")
    else:
        print("No data to evaluate")

# First install sacrebleu if not already installed
!pip install sacrebleu>=2.3.0
!pip install pandas # Install pandas if not already present

# Then run the evaluation
from pathlib import Path
import json
import pandas as pd # Import pandas
from sacrebleu.metrics import CHRF

def _extract(line: str) -> str:
    try:
        obj = json.loads(line) if line.strip().startswith(("[", "{", "\"")) else [line]
        return obj[0] if isinstance(obj, list) else obj.get("translation", "")
    except json.JSONDecodeError:
        # If it's not valid JSON, just return the stripped line
        return line.strip()
    except Exception:
        # Catch other potential errors
        return line.strip()


def _load(path: Path) -> List[str]:
    texts = []
    if path.suffix.lower() == '.csv':
        try:
            df = pd.read_csv(path)
            if 'pred_txt' in df.columns:
                texts = df['pred_txt'].astype(str).tolist() # Ensure texts are strings
            else:
                print(f"Error: CSV file {path} does not contain a 'pred_txt' column.")
        except FileNotFoundError:
            print(f"File not found: {path}")
        except Exception as e:
            print(f"Error reading CSV file {path}: {e}")
    elif path.suffix.lower() == '.jsonl':
        try:
            with path.open(encoding="utf-8") as f:
                for ln in f:
                    if ln.strip():
                        texts.append(_extract(ln))
        except FileNotFoundError:
            print(f"File not found: {path}")
        except Exception as e:
            print(f"Error reading JSONL file {path}: {e}")
    else:
        print(f"Unsupported file format for {path}. Please use .csv or .jsonl")

    return texts

# Set your file paths here
ref_file = "/tmp/pralekha_data/dev/eng_hin/doc.hin.jsonl"  # Reference file - Corrected path
hyp_file = "/content/gemma3_1b_it_sentence_translations_eng_hin_dev_10.csv"  # Your model output - Corrected path and extension

# Load files
refs = _load(Path(ref_file))
hyps = _load(Path(hyp_file))

print(f"References: {len(refs)}, Hypotheses: {len(hyps)}")

if not refs or not hyps:
    print("Error loading files. Cannot compute score.")
elif len(refs) != len(hyps):
    print(f"Warning: Length mismatch! refs={len(refs)} vs hyps={len(hyps)}")
    # Use the minimum length to avoid errors
    min_len = min(len(refs), len(hyps))
    refs = refs[:min_len]
    hyps = hyps[:min_len]
    print(f"Using first {min_len} samples for evaluation")


    # Compute ChrF score
    score = CHRF().corpus_score(hyps, [refs]).score
    print(f"ChrF Score: {score:.4f}")

    # Save to file if needed
    try:
        with open("/content/scores.tsv", "a") as f:
            f.write(f"{Path(hyp_file).name}\t{score:.4f}\n")
        print("Score saved to /content/scores.tsv")
    except Exception as e:
        print(f"Error saving score to file: {e}")
else:
     # Compute ChrF score when lengths match
    score = CHRF().corpus_score(hyps, [refs]).score
    print(f"ChrF Score: {score:.4f}")

    # Save to file if needed
    try:
        with open("/content/scores.tsv", "a") as f:
            f.write(f"{Path(hyp_file).name}\t{score:.4f}\n")
        print("Score saved to /content/scores.tsv")
    except Exception as e:
        print(f"Error saving score to file: {e}")

# First install required packages #EVALUATION CODE#
!pip install sacrebleu>=2.3.0
!pip install pandas openpyxl

# Then run the evaluation
from pathlib import Path
import json
import pandas as pd
from sacrebleu.metrics import CHRF
import os

def _extract(line: str) -> str:
    try:
        obj = json.loads(line) if line.strip().startswith(("[", "{", "\"")) else [line]
        return obj[0] if isinstance(obj, list) else obj.get("translation", "")
    except json.JSONDecodeError:
        return line.strip()
    except Exception:
        return line.strip()

def _load(path: Path) -> List[str]:
    texts = []
    if path.suffix.lower() == '.csv':
        try:
            df = pd.read_csv(path)
            if 'pred_txt' in df.columns:
                texts = df['pred_txt'].astype(str).tolist()
            else:
                print(f"Error: CSV file {path} does not contain a 'pred_txt' column.")
        except FileNotFoundError:
            print(f"File not found: {path}")
        except Exception as e:
            print(f"Error reading CSV file {path}: {e}")
    elif path.suffix.lower() == '.jsonl':
        try:
            with path.open(encoding="utf-8") as f:
                for ln in f:
                    if ln.strip():
                        texts.append(_extract(ln))
        except FileNotFoundError:
            print(f"File not found: {path}")
        except Exception as e:
            print(f"Error reading JSONL file {path}: {e}")
    else:
        print(f"Unsupported file format for {path}. Please use .csv or .jsonl")
    return texts

# ---------------------------------------------------------------------------
# EVALUATE ALL LANGUAGE PAIRS
# ---------------------------------------------------------------------------

LANGUAGE_PAIRS = [
    # English to Indic languages
    ("eng", "ben"), ("eng", "guj"), ("eng", "hin"), ("eng", "kan"),
    ("eng", "mal"), ("eng", "mar"), ("eng", "ori"), ("eng", "pan"),
    ("eng", "tam"), ("eng", "tel"), ("eng", "urd"),

    # Indic languages to English (reverse direction)
    ("ben", "eng"), ("guj", "eng"), ("hin", "eng"), ("kan", "eng"),
    ("mal", "eng"), ("mar", "eng"), ("ori", "eng"), ("pan", "eng"),
    ("tam", "eng"), ("tel", "eng"), ("urd", "eng")
]

LANG_NAMES = {
    "eng": "English", "ben": "Bengali", "guj": "Gujarati", "hin": "Hindi",
    "kan": "Kannada", "mal": "Malayalam", "mar": "Marathi", "ori": "Odia",
    "pan": "Punjabi", "tam": "Tamil", "tel": "Telugu", "urd": "Urdu"
}

def evaluate_all_languages():
    """Evaluate all language pairs and save to Excel & CSV"""
    SUBSET = "dev"
    MODEL_TYPE = "it"
    STRATEGY = "sentence"
    N = 10

    print("Starting evaluation for ALL language pairs...")

    # Create a DataFrame to store all results
    results_df = pd.DataFrame(columns=['Language Pair', 'Direction', 'ChrF Score', 'Model Type', 'Samples', 'File Name'])

    for src_lang, tgt_lang in LANGUAGE_PAIRS:
        print(f"\n{'='*60}")
        print(f"Evaluating: {LANG_NAMES[src_lang]} → {LANG_NAMES[tgt_lang]}")
        print(f"{'='*60}")

        # Construct file paths
        hyp_file = f"gemma3_1b_{MODEL_TYPE}_{STRATEGY}_{src_lang}_to_{tgt_lang}_{SUBSET}_{N}.csv"
        ref_file = f"/tmp/pralekha_data/{SUBSET}/{src_lang}_{tgt_lang}/doc.{tgt_lang}.jsonl"

        print(f"Hypothesis: {hyp_file}")
        print(f"Reference: {ref_file}")

        # Load files
        refs = _load(Path(ref_file))
        hyps = _load(Path(hyp_file))

        print(f"References: {len(refs)}, Hypotheses: {len(hyps)}")

        if not refs or not hyps:
            print("Error loading files. Skipping...")
            continue

        if len(refs) != len(hyps):
            print(f"Warning: Length mismatch! refs={len(refs)} vs hyps={len(hyps)}")
            min_len = min(len(refs), len(hyps))
            refs = refs[:min_len]
            hyps = hyps[:min_len]
            print(f"Using first {min_len} samples for evaluation")

        # Compute ChrF score
        try:
            score = CHRF().corpus_score(hyps, [refs]).score
            print(f"ChrF Score: {score:.4f}")

            # Add to results DataFrame
            new_row = pd.DataFrame({
                'Language Pair': [f"{src_lang}-{tgt_lang}"],
                'Direction': [f"{LANG_NAMES[src_lang]} → {LANG_NAMES[tgt_lang]}"],
                'ChrF Score': [score],
                'Model Type': [MODEL_TYPE],
                'Samples': [len(hyps)],
                'File Name': [hyp_file]
            })

            results_df = pd.concat([results_df, new_row], ignore_index=True)

        except Exception as e:
            print(f"Error computing ChrF score: {e}")
            continue

    return results_df

# ---------------------------------------------------------------------------
# RUN EVALUATION AND SAVE RESULTS
# ---------------------------------------------------------------------------

# Evaluate all language pairs
results_df = evaluate_all_languages()

# Save to CSV file
csv_file = "translation_evaluation_results.csv"
results_df.to_csv(csv_file, index=False)
print(f"\n✓ Results saved to CSV: {csv_file}")

# Save to Excel file
excel_file = "translation_evaluation_results.xlsx"
with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
    # Main results sheet
    results_df.to_excel(writer, sheet_name='Evaluation Results', index=False)

    # Summary statistics sheet
    summary_df = results_df.groupby('Direction')['ChrF Score'].agg(['mean', 'std', 'count']).round(4)
    summary_df.to_excel(writer, sheet_name='Summary Statistics')

    # Pivot table sheet
    pivot_df = results_df.pivot_table(
        values='ChrF Score',
        index='Language Pair',
        columns='Model Type',
        aggfunc='mean'
    ).round(4)
    pivot_df.to_excel(writer, sheet_name='Pivot Table')

print(f"✓ Results saved to Excel: {excel_file}")

# Print summary
print(f"\n{'='*60}")
print("EVALUATION SUMMARY:")
print(f"{'='*60}")
print(results_df[['Direction', 'ChrF Score', 'Samples']].to_string(index=False))

print(f"\n{'='*60}")
print("✓ Evaluation completed! Files created:")
print(f"1. {csv_file} (CSV format)")
print(f"2. {excel_file} (Excel format with multiple sheets)")
print(f"{'='*60}")

# Show the files in current directory
print("\nFiles in current directory:")
!ls -la *.csv *.xlsx 2>/dev/null || echo "No CSV or Excel files found"